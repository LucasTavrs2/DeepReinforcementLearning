{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f8231d",
   "metadata": {},
   "source": [
    "# Definição do Markov Decision Problem (MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "# States\n",
    "S = ['1', '2', '3']\n",
    "\n",
    "# Actions\n",
    "A = ['L', 'R']\n",
    "\n",
    "# Transition probabilities\n",
    "L = np.array([[1.0, 0.0, 0.0],\n",
    "              [0.8, 0.2, 0.0],\n",
    "              [0.0, 0.8, 0.2]])\n",
    "\n",
    "R = np.array([[0.2, 0.8, 0.0],\n",
    "              [0.0, 0.2, 0.8],\n",
    "              [0.0, 0.0, 1.0]])\n",
    "\n",
    "P = [L, R]\n",
    "\n",
    "# Reward function\n",
    "R = np.array([[0.0, 0.0],\n",
    "              [0.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "\n",
    "gamma = 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dfcbde",
   "metadata": {},
   "source": [
    "# Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize V\n",
    "V = np.ones(len(S))\n",
    "\n",
    "err = 1\n",
    "i = 0\n",
    "\n",
    "while err > 1e-2:\n",
    "    Q = []\n",
    "\n",
    "    # Compute Q-values associated with each action\n",
    "    for a in range(len(A)):\n",
    "        Q += [R[:, a] + gamma * P[a].dot(V)]\n",
    "    print('Q values at time', i)\n",
    "    print(Q)\n",
    "\n",
    "    # Compute maximum for each state\n",
    "    Vnew = np.max(Q, axis=0)\n",
    "    print('V-function at time', i)\n",
    "    print(Vnew)\n",
    "\n",
    "    # Compute error\n",
    "    err = np.linalg.norm(V - Vnew)\n",
    "\n",
    "    # Update V\n",
    "    V = Vnew\n",
    "\n",
    "    i += 1\n",
    "\n",
    "print('\\nV* =')\n",
    "print(V[:, None])\n",
    "\n",
    "print('\\nQ* =')\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d430d",
   "metadata": {},
   "source": [
    "# Função de avaliação de política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Rw, P, pol):\n",
    "    \"\"\"\n",
    "    evaluate(Rw, P, pol) computes the state–value function associated with policy pol.\n",
    "\n",
    "    :param Rw: 2D np.ndarray with |S| rows and |A| columns; element Rw[s,a] is the reward of action a in state s\n",
    "    :param P: 3D np.ndarray with |A| |S| x |S| matrices; matrix P[a] is the transition matrix for action a\n",
    "    :param pol: 2D np.ndarray with same dimensions as Rw; element pol[s,a] is the probability of action a in state s\n",
    "    :return V: 2D np.ndarray with |S| elements, where element s is Vpi(s).\n",
    "    \"\"\"\n",
    "    # Problem dimensions\n",
    "    nS, nA = Rw.shape\n",
    "\n",
    "    # Policy-averaged reward\n",
    "    Rpi = (pol * Rw).sum(axis=1)\n",
    "    print('Rpi =')\n",
    "    print(Rpi)\n",
    "\n",
    "    # Policy-averaged probabilities\n",
    "    Ppi = pol[:, 0, None] * P[0]\n",
    "    for a in range(1, nA):\n",
    "        Ppi += pol[:, a, None] * P[a]\n",
    "\n",
    "    print('Ppi =')\n",
    "    print(Ppi)\n",
    "\n",
    "    # Use matrix inversion to compute Vpi\n",
    "    Vpi = np.linalg.inv(np.eye(nS) - gamma * Ppi).dot(Rpi)\n",
    "\n",
    "    return Vpi[:, None]\n",
    "# -- End: evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cecd36",
   "metadata": {},
   "source": [
    "# Laço principal da Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ee76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Initialize policy (uniform random)\n",
    "pol = np.ones((len(S), len(A))) / len(A)\n",
    "\n",
    "print('Initial policy =')\n",
    "print(pol)\n",
    "\n",
    "# Auxiliary matrix to store temporary Q-values\n",
    "Q = np.zeros((len(S), len(A)))\n",
    "\n",
    "quit = False\n",
    "i = 0\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "while not quit:\n",
    "    # Evaluate policy\n",
    "    V = evaluate(Rw, P, pol)\n",
    "    print('V =')\n",
    "    print(V)\n",
    "\n",
    "    # Compute Q-values\n",
    "    for a in range(len(A)):\n",
    "        Q[:, a, None] = Rw[:, a, None] + gamma * P[a].dot(V)\n",
    "\n",
    "    print('Q =')\n",
    "    print(Q)\n",
    "\n",
    "    # Compute maximizing policy\n",
    "    Qmax = Q.max(axis=1, keepdims=True)\n",
    "    polnew = np.isclose(Q, Qmax, atol=1e-10, rtol=1e-10).astype(int)\n",
    "    polnew = polnew / polnew.sum(axis=1, keepdims=True)\n",
    "\n",
    "    print('Policy =')\n",
    "    print(polnew)\n",
    "\n",
    "    # Check for convergence\n",
    "    quit = (pol == polnew).all()\n",
    "    pol = polnew\n",
    "    i += 1\n",
    "\n",
    "t = time.time() - t\n",
    "\n",
    "print('N. iterations:', i)\n",
    "print('Time taken:', round(t, 3), 'seconds')\n",
    "print('\\npi* =')\n",
    "print(pol)\n",
    "print('\\nV* =')\n",
    "print(V[:, None])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
